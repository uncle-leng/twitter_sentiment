{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('…', 654), ('’', 296), ('...', 114), ('get', 110), ('like', 108), ('go', 83), ('one', 76), ('see', 69), ('time', 66), ('new', 64), ('look', 62), ('😂', 61), ('think', 60), ('thank', 59), ('love', 57), ('u', 57), ('work', 56), ('great', 56), ('say', 51), ('day', 51)]\n",
      "CPU times: user 5.38 s, sys: 211 ms, total: 5.59 s\n",
      "Wall time: 5.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import re\n",
    "import string\n",
    "from couchdb import Server\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from datetime import datetime\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import bigrams \n",
    "from nltk import trigrams\n",
    "from textblob import TextBlob\n",
    "\n",
    "# server = Server('http://43.240.96.132:5984/')\n",
    "\n",
    "# db = server['testdb']\n",
    "# print(type(db))\n",
    "# print(len(db))\n",
    "\n",
    "# f = open('tweets.txt', 'w+')\n",
    "\n",
    "# count = 0\n",
    "# for doc_id in db:\n",
    "#     text = db[doc_id]['text']\n",
    "#     text = re.sub('\\n+',' ',text)\n",
    "#     time = db[doc_id]['created_at']\n",
    "#     f.write('{}<=>{}\\n'.format(text, time))\n",
    "#     count += 1\n",
    "#     if count >= 10000:\n",
    "#         break\n",
    "\n",
    "punctuation = set(string.punctuation)\n",
    "# stop words\n",
    "stop = set(stopwords.words('english')).union(punctuation)\n",
    "\n",
    "tweets = []\n",
    "# read data from file\n",
    "with open('tweets.txt') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            string = line.split('<=>')\n",
    "            text = string[0]\n",
    "            time = string[1].strip()\n",
    "            tweets.append([text, time])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "time_format = '%a %b %d %H:%M:%S %z %Y'\n",
    "time_zone = []\n",
    "for i in range(0, 24, 2):\n",
    "    time_zone.append([i, i+2])\n",
    "\n",
    "# set the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "def lemmatize(text):\n",
    "    lemma = lemmatizer.lemmatize(text,'v')\n",
    "    if lemma == text:\n",
    "        lemma = lemmatizer.lemmatize(text,'n')\n",
    "    return lemma\n",
    "\n",
    "# set the tokenizer for tweets\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "# dict sort time_zone and words pairs\n",
    "# {(0,2): {word1: 2, word2: 1, ...}, (2,4):..., ....}\n",
    "time_words = {}\n",
    "\n",
    "for i in tweets:\n",
    "    time_object = datetime.strptime(i[1], time_format)\n",
    "    time = time_object.hour + float(time_object.minute / 60)\n",
    "    for zone in time_zone:\n",
    "        if zone[0] <= time <= zone[1]:\n",
    "            tokens = tknzr.tokenize(i[0])\n",
    "            for j in tokens:\n",
    "                word = lemmatize(j.lower())\n",
    "                if word in stop or word[:5] == 'https' or word[:4] == 'http':\n",
    "                    continue\n",
    "                else:\n",
    "                    word_dict = time_words.get((zone[0], zone[1]), {})\n",
    "                    word_dict[word] = word_dict.get(word, 0) + 1\n",
    "                    time_words[(zone[0], zone[1])] = word_dict\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "def getHotWord(time_zone, time_word_dict):\n",
    "    '''\n",
    "        The data type of time_zone is tuple\n",
    "        e.g. (2, 4) means from 2-4 in 24H\n",
    "    '''\n",
    "    dict = time_word_dict[time_zone]\n",
    "    sorted_list = sorted(dict.items(), key=lambda x:x[1], reverse=True)\n",
    "    return sorted_list[:20]\n",
    "\n",
    "# zone set to (0,2)\n",
    "hot_word = getHotWord((0,2), time_words)\n",
    "print(hot_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('of', 'the'), 59), (('in', 'the'), 53), (('for', 'the'), 38), (('on', 'the'), 34), (('is', 'a'), 34), (('for', 'a'), 31), (('to', 'be'), 28), (('at', 'the'), 28), (('to', 'the'), 26), (('I', 'just'), 21), (('have', 'to'), 20), (('I', 'have'), 19), (('is', 'the'), 18), (('with', 'the'), 18), (('via', '@YouTube'), 17), (('a', 'great'), 17), (('a', '@YouTube'), 17), (('one', 'of'), 17), (('This', 'is'), 16), (('have', 'a'), 16)]\n"
     ]
    }
   ],
   "source": [
    "bigram_time_words = {}\n",
    "\n",
    "for i in tweets:\n",
    "    time_object = datetime.strptime(i[1], time_format)\n",
    "    time = time_object.hour + float(time_object.minute / 60)\n",
    "    sentence = i[0].split()\n",
    "    bigram = list(bigrams(sentence))\n",
    "    for zone in time_zone:\n",
    "        if zone[0] <= time <= zone[1]:\n",
    "            for gram in bigram:\n",
    "                word_dict = bigram_time_words.get((zone[0], zone[1]), {})\n",
    "                word_dict[gram] = word_dict.get(gram, 0) + 1\n",
    "                bigram_time_words[(zone[0], zone[1])] = word_dict\n",
    "\n",
    "\n",
    "bigram_hot_word = getHotWord((0,2), bigram_time_words)\n",
    "print(bigram_hot_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23864\n"
     ]
    }
   ],
   "source": [
    "print(len(bigram_time_words[(0,2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('I', 'liked', 'a'), 15), (('liked', 'a', '@YouTube'), 15), (('a', '@YouTube', 'video'), 15), (('BONG!', 'BONG!', 'BONG!'), 10), (('DING', 'DING', 'DING'), 10), (('one', 'of', 'the'), 9), (('lol', 'lol', 'lol'), 6), (('you', 'have', 'to'), 5), (('I', 'love', 'the'), 5), (('a', 'lot', 'of'), 5), (('going', 'to', 'be'), 5), (('thanks', 'for', 'the'), 5), (('This', 'is', 'a'), 5), (('Looking', 'forward', 'to'), 5), (('I', 'need', 'to'), 4), (('just', 'want', 'to'), 4), (('of', 'the', 'best'), 4), (('@australian', '@sallyrugg', '@yassmin_a'), 4), (('@The_Real_BiM', '@radicapitalist', '@ChadCottle…'), 4), (('https://t.co/2QWGIs8JwY', '#funkykidsradio', '#music4kids'), 4)]\n"
     ]
    }
   ],
   "source": [
    "trigram_time_words = {}\n",
    "\n",
    "for i in tweets:\n",
    "    time_object = datetime.strptime(i[1], time_format)\n",
    "    time = time_object.hour + float(time_object.minute / 60)\n",
    "    sentence = i[0].split()\n",
    "    trigram = list(trigrams(sentence))\n",
    "    for zone in time_zone:\n",
    "        if zone[0] <= time <= zone[1]:\n",
    "            for gram in trigram:\n",
    "                word_dict = trigram_time_words.get((zone[0], zone[1]), {})\n",
    "                word_dict[gram] = word_dict.get(gram, 0) + 1\n",
    "                trigram_time_words[(zone[0], zone[1])] = word_dict\n",
    "\n",
    "trigram_hot_word = getHotWord((0,2), trigram_time_words)\n",
    "print(trigram_hot_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24469\n"
     ]
    }
   ],
   "source": [
    "print(len(trigram_time_words[(0,2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theuntzpodcast ethanglassmusic he got shucker punched\n",
      "0.40642458100558687\n",
      "Kiama’s popularity appears set to continue as a “top end” Hamptonsinspired property hits the market…\n",
      "0.5445573290292737\n",
      "Trust carefully\n",
      "0.5805716023107329\n",
      "Rivalm8 MF up 10\n",
      "0.5\n",
      "guys humidifiers are cool im in an office but i feel like im breathing fresh air from outside\n",
      "0.5139646576639431\n",
      "Camiliasilf Hope you do\n",
      "0.4921981004070555\n",
      "Hi hello i have broken out in a rash all over my body Everything burns\n",
      "0.02988286956907639\n",
      "ahhh bbb\n",
      "0.2499999999999997\n",
      "Telstra Hi Tim unfortunately I dont have time to deal with this right now so will have to wait until I get home…\n",
      "0.07500773467107623\n",
      "Can you see the little 8 legged creature we found this week Can you…\n",
      "0.5327828811146877\n",
      "Kidman Hugh Jackman top Time most influential 100 list\n",
      "0.5873215241205519\n",
      "XxKaRLyKiTTeNxX Pretty sure Id remember it Also it would be such a good comeback for Louis just saying\n",
      "0.6188829811519586\n",
      "SeadPetovic Saints are gonna be like James harden and be 218 this year after 20 rounds\n",
      "0.8274993916828883\n",
      "you have to shriek until the ball lands otherwise its a penalty shot\n",
      "0.359687438887141\n",
      "\n",
      "0.5\n",
      "XxKaRLyKiTTeNxX nah just chill they can wait\n",
      "0.3429742256850373\n",
      "DerekBell21 RealGDT Your creature collection🔥💯\n",
      "0.5147686200154055\n",
      "Interview on WIN TV about ChippersSchool kids holiday camp\n",
      "0.8472875307399478\n",
      "lilchip0tle Same Except bacon\n",
      "0.43563733882819955\n",
      "News facepalm\n",
      "0.4619565217391304\n",
      "CPU times: user 3.81 s, sys: 200 ms, total: 4.01 s\n",
      "Wall time: 4.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# sentiment analysis----------------\n",
    "\n",
    "from textblob import Blobber\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "import string\n",
    "\n",
    "punctuation = set(string.punctuation)\n",
    "pattern = re.compile('(@)|(#)')\n",
    "\n",
    "tb = Blobber(analyzer=NaiveBayesAnalyzer())\n",
    "\n",
    "for i in tweets[:20]:\n",
    "#     print(i[0])\n",
    "#     print(tb(i[0]).sentiment.p_pos)\n",
    "    # remove @ and # \n",
    "    tweet = re.sub(pattern,' ', i[0]).strip()\n",
    "    # remove http...\n",
    "    tweet = ' '.join([i for i in tweet.split() if not i.startswith('http')])\n",
    "    # remove puncutation\n",
    "    tweet = ''.join([i for i in tweet if i not in punctuation])\n",
    "    print(tweet)\n",
    "    print(tb(tweet).sentiment.p_pos)\n",
    "    \n",
    "\n",
    "# for i in tweets[:20]:\n",
    "#     testimonial = TextBlob(i[0])\n",
    "#     print(i[0])\n",
    "#     print(testimonial.sentiment)\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@theuntzpodcast @ethanglassmusic he got shucker punched', 'Fri Apr 20 01:50:51 +0000 2018']\n",
      "['Kiama’s popularity appears set to continue, as a “top end”, Hamptons-inspired property hits the market...… https://t.co/LnbuOBxDAt', 'Fri Apr 20 01:50:52 +0000 2018']\n",
      "['#Trust carefully https://t.co/Wxo8PMSJcY https://t.co/rzv8tDIxYL', 'Fri Apr 20 01:50:52 +0000 2018']\n",
      "['@Rivalm8 MF up 1-0', 'Fri Apr 20 01:50:52 +0000 2018']\n",
      "[\"guys humidifiers are cool. i'm in an office but i feel like i'm breathing fresh air from outside\", 'Fri Apr 20 01:50:53 +0000 2018']\n",
      "['@Camiliasilf Hope you do', 'Fri Apr 20 01:50:53 +0000 2018']\n",
      "['Hi, hello, i have broken out in a rash all over my body. Everything burns https://t.co/BR3XijnuhD', 'Fri Apr 20 01:50:54 +0000 2018']\n",
      "['ahhh bbb', 'Fri Apr 20 01:50:54 +0000 2018']\n",
      "[\"@Telstra Hi Tim, unfortunately I don't have time to deal with this right now, so will have to wait until I get home… https://t.co/LmrfRnT7tC\", 'Fri Apr 20 01:50:55 +0000 2018']\n",
      "['Can you see the little 8 legged creature we found this week? Can you… https://t.co/JfsdyAU2Yy', 'Fri Apr 20 01:50:55 +0000 2018']\n",
      "['https://t.co/QZiTZ9ETnG–Nicole Kidman, Hugh Jackman top Time most influential 100\\xa0list https://t.co/MXj5jd6Z5m', 'Fri Apr 20 01:50:56 +0000 2018']\n",
      "[\"@XxKaRLyKiTTeNxX Pretty sure I'd remember it. Also it would be such a good comeback for Louis just saying..\", 'Fri Apr 20 01:50:56 +0000 2018']\n",
      "['@SeadPetovic Saints are gonna be like James harden and be 2-18 this year after 20 rounds', 'Fri Apr 20 01:50:56 +0000 2018']\n",
      "[\"you have to shriek until the ball lands otherwise it's a penalty shot https://t.co/GyplxNSKZY\", 'Fri Apr 20 01:50:56 +0000 2018']\n",
      "['https://t.co/7X37J6k0IG https://t.co/XuKiSylPmE', 'Fri Apr 20 01:50:56 +0000 2018']\n",
      "['@XxKaRLyKiTTeNxX nah just chill they can wait', 'Fri Apr 20 01:50:57 +0000 2018']\n",
      "['@DerekBell21 @RealGDT Your creature collection🔥💯', 'Fri Apr 20 01:50:57 +0000 2018']\n",
      "['Interview on WIN TV about @ChippersSchool kids holiday camp https://t.co/Clr9W0GCdQ', 'Fri Apr 20 01:50:58 +0000 2018']\n",
      "['@lil_chip0tle Same. Except bacon...', 'Fri Apr 20 01:50:58 +0000 2018']\n",
      "['\"News\" *facepalm* https://t.co/fhrxYcvBQK', 'Fri Apr 20 01:50:58 +0000 2018']\n",
      "['@Michelavittori5 @JamesMarstersOf @WhedonCon @Comicpalooza @WizardWorld @FandemicTour @FanboyExpo @Mtlcomiccon… https://t.co/DWEnqYTeWf', 'Fri Apr 20 01:50:58 +0000 2018']\n",
      "['Javale McGee (murdered by Rudy Gay) headed to locker room', 'Fri Apr 20 01:50:59 +0000 2018']\n",
      "['Not the same article I posted about earlier today. @9NewsAUS posted another one after he was convicted.  #FixedIt… https://t.co/n3CsqUPeVC', 'Fri Apr 20 01:50:59 +0000 2018']\n",
      "['@ray_card @Barnaby_Joyce You’re just annoyed because I called out Hawkins for being a plodder #truth-hurts', 'Fri Apr 20 01:50:59 +0000 2018']\n",
      "['@Band_TheRose LEGENDS!!!!', 'Fri Apr 20 01:50:59 +0000 2018']\n",
      "['Implications of differences in employment estimates by industry between ABS Labour Force Survey and ABS Business-su… https://t.co/HycEOJ3dFE', 'Fri Apr 20 01:50:59 +0000 2018']\n",
      "['@musicnewsfact No Tears Left To Cry 💧', 'Fri Apr 20 01:50:59 +0000 2018']\n",
      "['How is this at all relevant? Shit like this makes me wanna support Trump. https://t.co/V2C0XVPTHd', 'Fri Apr 20 01:51:00 +0000 2018']\n",
      "['The fastest car in the world? - Bugatti Veyron Grand Sport displayed at ... https://t.co/nrQvvQGrHD via @YouTube', 'Fri Apr 20 01:51:00 +0000 2018']\n",
      "['Book this fantastic specimen. https://t.co/2UV3PjnZe1', 'Fri Apr 20 01:51:00 +0000 2018']\n",
      "[\"We're hiring! Our Melbourne team is looking for PHP developers to work on some great #OpenSource projects. https://t.co/qSBpUNaGBO\", 'Fri Apr 20 01:51:00 +0000 2018']\n",
      "['https://t.co/4pDtb4sL3b', 'Fri Apr 20 01:51:00 +0000 2018']\n",
      "['Stairway to my heaven❔☁️ . . #stairway #heavensgate #milsonspoint #concerts #augustalsina… https://t.co/7J6le8UqWr', 'Fri Apr 20 01:51:00 +0000 2018']\n",
      "['And I swear I will throttle the next person who tells me I need to drink water.', 'Fri Apr 20 01:51:01 +0000 2018']\n",
      "['@7NewsAdelaide @olivialeeming @GemmaActon The review into inapprpriate financial practices in the banking sector is… https://t.co/YYLW0NXnIa', 'Fri Apr 20 01:51:01 +0000 2018']\n",
      "['IPOs worth $1bn to hit market https://t.co/xdNtkV3HyX', 'Fri Apr 20 01:51:02 +0000 2018']\n",
      "['Give away time!!!!!! https://t.co/fQzUyo7MsD', 'Fri Apr 20 01:51:02 +0000 2018']\n",
      "['#springst https://t.co/gBhIqh0Xxj', 'Fri Apr 20 01:51:02 +0000 2018']\n",
      "['Great speech about looking at both sides of an argument. Crucial in this time of #echochamber social media. 5 sta… https://t.co/0PN5flOMkX', 'Fri Apr 20 01:51:03 +0000 2018']\n",
      "['Breakwater Rd  (C124), Belmont - Traffic Alert, Truck breakdown near Settlement Rd. Left lane closed… https://t.co/mzwhbEEvAW #victraffic', 'Fri Apr 20 01:51:03 +0000 2018']\n",
      "['@TStew777 Looks like Coles have some standards after all', 'Fri Apr 20 01:51:03 +0000 2018']\n",
      "['Huge win from the @sixers Coming of age stuff. #NBAPlayoffs', 'Fri Apr 20 01:51:04 +0000 2018']\n",
      "['Breaking news, CUB ruin fucking everything.', 'Fri Apr 20 01:51:04 +0000 2018']\n",
      "['@TheMightyLangaz Have you seen the comments on the nine news Australia tweet ?', 'Fri Apr 20 01:51:04 +0000 2018']\n",
      "['@centremf @Phillyronan  https://t.co/iTWOdhZHvH', 'Fri Apr 20 01:51:04 +0000 2018']\n",
      "[\"I'm sorry but they completely fucked up an opportunity to release this on May the 4th https://t.co/0w1waS65fS\", 'Fri Apr 20 01:51:05 +0000 2018']\n",
      "['Homophobia is a public health issue, contributing to higher suicide rates in the LGBTQI community. Unlike extending… https://t.co/8ZtwLg1Hhz', 'Fri Apr 20 01:51:05 +0000 2018']\n",
      "['I do love the great respect and admiration the Oz constantly conveys to @yassmin_a by breathlessly reporting on eve… https://t.co/1HzU8r78YY', 'Fri Apr 20 01:51:05 +0000 2018']\n",
      "['JOB AVAILABLE: Duty Manager THE COMPANY: Corrimal RSL Memorial Club, Corrimal, NSW APPLY: https://t.co/vImnbcOYOO O… https://t.co/CckUVsyf4D', 'Fri Apr 20 01:51:05 +0000 2018']\n",
      "['Four Little Letters (2): https://t.co/UFNV2ZegJu via @YouTube', 'Fri Apr 20 01:51:05 +0000 2018']\n",
      "\n",
      "\n",
      "(0, '0.324*\"creature\" + 0.324*\"win\" + 0.176*\"little\" + 0.029*\"great\" + 0.029*\"time\" + 0.029*\"everything\" + 0.029*\"news\"')\n",
      "(1, '0.379*\"hi\" + 0.207*\"everything\" + 0.207*\"time\" + 0.034*\"great\" + 0.034*\"news\" + 0.034*\"win\" + 0.034*\"little\"')\n",
      "(2, '0.430*\"little\" + 0.071*\"great\" + 0.071*\"time\" + 0.071*\"everything\" + 0.071*\"news\" + 0.071*\"win\" + 0.071*\"creature\"')\n",
      "(3, '0.327*\"news\" + 0.327*\"time\" + 0.122*\"everything\" + 0.122*\"great\" + 0.020*\"win\" + 0.020*\"little\" + 0.020*\"creature\"')\n",
      "(4, '0.470*\"im\" + 0.325*\"great\" + 0.029*\"time\" + 0.029*\"news\" + 0.029*\"everything\" + 0.029*\"win\" + 0.029*\"little\"')\n",
      "CPU times: user 672 ms, sys: 15.4 ms, total: 687 ms\n",
      "Wall time: 685 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# topic modelling----------------\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "\n",
    "punctuation = set(string.punctuation)\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "corpus = []\n",
    "pattern = re.compile('(#)|(@[A-Za-z0-9]+)')\n",
    "\n",
    "for i in tweets[:50]:\n",
    "    # remove @ and # \n",
    "    tweet = re.sub(pattern,' ', i[0]).strip()\n",
    "    print(i)\n",
    "    # remove http...\n",
    "    tweet = [i for i in tweet.lower().split() if not i.startswith('http')]\n",
    "    \n",
    "    # remove verb\n",
    "    pos_tag = nltk.pos_tag(tweet)\n",
    "    tweet = [i[0] for i in pos_tag if (not i[1].startswith('V')) and (i[1] != 'IN') ]\n",
    "    \n",
    "    # remove stop words\n",
    "    tweet = \" \".join([i for i in tweet if i not in stop])\n",
    "    # remove puncutation\n",
    "    tweet = ''.join([i for i in tweet if i not in punctuation])\n",
    "    # normalise the word\n",
    "    tweet = [lemmatize(word) for word in tweet.split()]\n",
    "   \n",
    "    corpus.append(tweet)\n",
    "    # print(tweet)\n",
    "\n",
    "# for i in corpus:\n",
    "#     print(i)\n",
    "\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index.\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "\n",
    "# filter the extreme tokens\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.9)\n",
    "\n",
    "# Converting list of corpus into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=5, id2word = dictionary, passes=50, dtype=np.float64)\n",
    "\n",
    "# get probability distribution for each tweet\n",
    "# probability = ldamodel.get_document_topics(doc_term_matrix)\n",
    "# print(len(probability))\n",
    "# for i in probability:\n",
    "#     print(i)\n",
    "\n",
    "print('\\n')\n",
    "for i in ldamodel.show_topics(num_words=7):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # analysis the most tweeting user\n",
    "# import draft\n",
    "\n",
    "# # get top 10 users\n",
    "# users = draft.get_top_n_user(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'key': 256478435, 'value': 2464}, {'key': 931566656115032000, 'value': 1211}, {'key': 3958911379, 'value': 1093}, {'key': 979984755125928000, 'value': 958}, {'key': 133042870, 'value': 871}, {'key': 971914328982011900, 'value': 823}, {'key': 57314631, 'value': 746}, {'key': 2427740995, 'value': 717}, {'key': 378439995, 'value': 686}, {'key': 347883924, 'value': 655}]\n",
      "getting response from the server...\n"
     ]
    }
   ],
   "source": [
    "# get user tweets\n",
    "print(users)\n",
    "# 133042870\n",
    "user_tweets = draft.get_user_tweets(971914328982011900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "(0, '0.032*\"you\" + 0.026*\"right\" + 0.020*\"that’s\" + 0.012*\"also\" + 0.012*\"good\" + 0.012*\"time\" + 0.011*\"that\"')\n",
      "(1, '0.036*\"russia\" + 0.022*\"syria\" + 0.018*\"u\" + 0.016*\"belgium\" + 0.016*\"russian\" + 0.012*\"people\" + 0.010*\"it’s\"')\n",
      "(2, '0.032*\"video\" + 0.029*\"official\" + 0.021*\"love\" + 0.020*\"so\" + 0.018*\"—\" + 0.015*\"word\" + 0.015*\"michelle\"')\n",
      "(3, '0.021*\"—\" + 0.015*\"this\" + 0.015*\"still\" + 0.014*\"never\" + 0.012*\"even\" + 0.012*\"way\" + 0.011*\"well\"')\n",
      "(4, '0.023*\"would\" + 0.019*\"much\" + 0.017*\"year\" + 0.012*\"past\" + 0.010*\"netherlands\" + 0.010*\"spain\" + 0.010*\"opcw\"')\n",
      "CPU times: user 15.5 s, sys: 30.5 ms, total: 15.5 s\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# user topic modelling----------------\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "punctuation = set(string.punctuation)\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# set the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "def lemmatize(text):\n",
    "    lemma = lemmatizer.lemmatize(text,'v')\n",
    "    if lemma == text:\n",
    "        lemma = lemmatizer.lemmatize(text,'n')\n",
    "    return lemma\n",
    "\n",
    "corpus = []\n",
    "pattern = re.compile('(#)|(@[A-Za-z0-9]+)')\n",
    "for i in user_tweets:\n",
    "    # remove @ and # \n",
    "    tweet = re.sub(pattern,' ', i).strip()\n",
    "    # remove http...\n",
    "    tweet = [i for i in tweet.lower().split() if not i.startswith('http')]\n",
    "    \n",
    "    # remove verb\n",
    "    pos_tag = nltk.pos_tag(tweet)\n",
    "    tweet = [i[0] for i in pos_tag if (not i[1].startswith('V')) and (i[1] != 'IN') ]\n",
    "    \n",
    "    # remove stop words\n",
    "    tweet = \" \".join([i for i in tweet if i not in stop])\n",
    "    # remove puncutation\n",
    "    tweet = ''.join([i for i in tweet if i not in punctuation])\n",
    "    # normalise the word\n",
    "    tweet = [lemmatize(word) for word in tweet.split()]\n",
    "    # tweet = [word for word in tweet.split()]\n",
    "    corpus.append(tweet)\n",
    "    # print(tweet)\n",
    "\n",
    "# for i in corpus:\n",
    "#     print(i)\n",
    "\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index.\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "\n",
    "# filter the extreme tokens\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.9)\n",
    "\n",
    "# Converting list of corpus into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=5, id2word = dictionary, passes=50, dtype=np.float64)\n",
    "\n",
    "# get probability distribution for each tweet\n",
    "# probability = ldamodel.get_document_topics(doc_term_matrix)\n",
    "# print(len(probability))\n",
    "# for i in probability:\n",
    "#     print(i)\n",
    "\n",
    "print('\\n')\n",
    "for i in ldamodel.show_topics(num_words=7):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kiama', '’', 's', 'popularity', 'appears', 'set', 'to', 'waaayyy', 'continue', ',', 'as', 'a', '“', 'top', 'end', '”', '!', '!', '!', ',', '#Hamptons-inspired', 'property', 'hits', 'the', 'market', '...', '…', 'https://t.co/LnbuOBxDAt']\n"
     ]
    }
   ],
   "source": [
    "cleanedTweet=' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|(RT)\", \" \", text).split())\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "s0 = 'Kiama’s popularity appears set to waaaayyyy continue, as a “top end”, #Hamptons-inspired property hits the market...… https://t.co/LnbuOBxDAt'\n",
    "a = tknzr.tokenize(s0)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
